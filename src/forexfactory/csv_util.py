# src/forexfactory/csv_util.py

import csv
import os
import pandas as pd
from datetime import datetime

import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Define the CSV columns
CSV_COLUMNS = ["DateTime", "Currency", "Impact", "Event", "Actual", "Forecast", "Previous", "Detail"]

def ensure_csv_header(csv_file):
    """
    Ensure that the CSV file exists with the proper header.
    """
    if not os.path.exists(csv_file):
        df = pd.DataFrame(columns=CSV_COLUMNS)
        df.to_csv(csv_file, index=False)


def read_existing_data(csv_file):
    """
    Read the existing CSV data and return a DataFrame with the defined columns.
    """
    if os.path.exists(csv_file):
        try:
            df = pd.read_csv(csv_file, dtype=str)
            # Ensure all columns exist in the DataFrame
            for col in CSV_COLUMNS:
                if col not in df.columns:
                    df[col] = ""
            return df[CSV_COLUMNS]
        except Exception as e:
            print(f"Error reading CSV: {e}")
            return pd.DataFrame(columns=CSV_COLUMNS)
    else:
        return pd.DataFrame(columns=CSV_COLUMNS)

def write_data_to_csv(df: pd.DataFrame, csv_file: str):
    """
    Write final merged data to CSV, overwriting it.
    """
    # sort the data by DateTime
    df = df.sort_values(by="DateTime", ascending=True)
    df.to_csv(csv_file, index=False)


def merge_new_data(existing_df, new_df):
    """
    Merge new data into the existing DataFrame.

    For each record in new_df:
      - If the record does not exist in existing_df, append it.
      - If the record exists:
          - If the existing record's 'Detail' field is empty and the new record contains details,
            update the 'Detail' field.
          - Otherwise, leave the record unchanged.

    A unique key is generated by concatenating DateTime, Currency, and Event.
    """
    if existing_df.empty:
        return new_df

    def add_unique_key(df):
        df = df.copy()
        df['unique_key'] = (
            df["DateTime"].astype(str).str.strip() + "_" +
            df["Currency"].astype(str).str.strip() + "_" +
            df["Event"].astype(str).str.strip()
        )
        return df

    existing_df = add_unique_key(existing_df)
    new_df = add_unique_key(new_df)

    # Use the unique key as the index for easy lookup
    existing_df.set_index('unique_key', inplace=True)
    new_df.set_index('unique_key', inplace=True)

    # Accumulate new rows that are not present in existing_df
    new_rows_list = []
    for key, new_row in new_df.iterrows():
        if key in existing_df.index:
            # Retrieve the 'Detail' field for comparison
            existing_detail = str(existing_df.at[key, "Detail"]).strip() if pd.notna(existing_df.at[key, "Detail"]) else ""
            new_detail = str(new_row["Detail"]).strip() if pd.notna(new_row["Detail"]) else ""
            # Update the 'Detail' field only if it is missing in the existing record
            # and if the new row contains detail data.
            if not existing_detail and new_detail:
                existing_df.at[key, "Detail"] = new_detail
        else:
            new_rows_list.append(new_row)

    if new_rows_list:
        new_rows_df = pd.DataFrame(new_rows_list)
        # Concatenate new rows with the existing DataFrame
        existing_df = pd.concat([existing_df, new_rows_df])

    # Reset the index and ensure the DataFrame has the original column order
    merged_df = existing_df.reset_index(drop=True)
    merged_df = merged_df[CSV_COLUMNS]
    return merged_df
